<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<style>
  body {
    padding: 100px;
    width: 1000px;
    margin: auto;
    text-align: left;
    font-weight: 300;
    font-family: 'Open Sans', sans-serif;
    color: #121212;
  }
  h1, h2, h3, h4 {
    font-family: 'Source Sans Pro', sans-serif;
  }
  th {
		font-weight: normal;
  }
</style>
<script>
  MathJax = {
      tex: { inlineMath: [['$', '$'], ['\\(', '\\)']] },
      svg: { fontCache: 'global' }
  };
</script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
</script>

<title>CS 184 Path Tracer</title>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link href="https://fonts.googleapis.com/css?family=Open+Sans|Source+Sans+Pro" rel="stylesheet">
</head>

<body>

<h1 align="middle">CS 184: Computer Graphics and Imaging, Spring 2023</h1>
<h1 align="middle">Project 3-1: Path Tracer</h1>
<h2 align="middle">Martin Hodde CS184-ABW, Kayla Kranen CS184-ACG</h2>

<br>
<div align="middle">
  <img src="images/part5-3.png" align="middle" width="500px"/>
</div>
<br>

<h2 align="middle">Overview</h2>
<p>
	In this project, we develop a path tracing program to simulate photorealistic illumination in 3D scenes, enabling us to render artificial images that approximately capture the same physical light that we observe in our visual reality. Further, we expedite the notoriously expensive path-tracing pipeline with hierarchical data structures and clever sampling techniques.
</p>
<p>
	Our path tracer assumes a ray model in which light travels through space along a straight line, subject to reflection off various physical surfaces, eventually landing on the eye of an observer. Hence, to render a scene from a certain perspective, we cast rays in the reverse direction—from the observer to the scene—and trace their paths to reason about the origin of the detected light and the intermediate surfaces it visited prior to reaching the observer.
</p>
<p>
	Granted that we accurately model the interaction between the rays and the surfaces we attempt to portray, this path-tracing procedure constructs a physically faithful estimation of a given visual experience that is (hopefully) indistinguishable from a photograph of its tangible counterpart.
</p>

<h2 align="middle">Part 1: Ray Generation and Scene Intersection</h2>
<p>
  To generate a single ray from the camera's perspective at some location, we transform its image coordinates to a ray in camera space that points from the camera origin to a point within an axis-aligned virtual camera sensor. We then apply a rotation matrix to transform this ray into one that exists in world space.
</p>
<p>
  For our path tracing to produce convincing output without excessive noise artifacts, we must generate several rays within each pixel. Thus, we sample random locations inside every pixel and cast a new ray for each sample into the scene. As we trace these paths through the scene, we estimate how much radiance they contribute to the output pixel, and update the sample buffer accordingly.
</p>
<p>
  Tracing the path of a ray, however, depends on our ability to model scene intersection. For the scope of this project, we consider intersections with triangles and spheres, updating intersection times, surface normals, and bidirectional scattering distribution functions (BSDFs) as we detect collisions.
</p>
<p>
  For ray-triangle intersection, we opted to implement the Möller-Trumbore algorithm, which involves computing a series of geometrically derived cross-products and dot-products that allow us to find the ray intersection time $t$ as well as express the ray in terms of barycentric coordinates $b_1$, $b_2$, and $b_0 = 1 - b_1 - b_2$ with respect to the vertices of the triangle. If all 3 barycentric coordinates are nonzero and $t$ lies within the range of observable values along the ray, then we indicate that an intersection has occurred. The Möller-Trumbore equations for finding the intersection time and barycentric coordinates are given below.
</p>

<br>
<div align="middle">
  <img src="images/triangle1.png" align="middle" width="420px"/>
</div>
<div align="middle">
  <table cellpadding="20">
    <tr>
      <td>
        <img src="images/triangle2.png" align="middle" height="225px"/>
      </td>
	  <td>
		<br>
	  </td>
      <td>
        <img src="images/triangle3.png" align="middle" height="140px"/>
      </td>
    </tr>
  </table>
</div>

<p>
	At this point, we are able to render simple images with smooth shading based on the surface normals we compute for intersected triangles and spheres.
</p>

<br>
<div align="middle">
  <table cellpadding="10">
    <tr>
      <td>
        <img src="images/part1-1.png" align="middle" width="300px"/>
        <figcaption align="middle">CB Spheres</figcaption>
      </td>
      <td>
        <img src="images/part1-2.png" align="middle" width="300px"/>
        <figcaption align="middle">CB Coil</figcaption>
      </td>
	  <td>
        <img src="images/part1-3.png" align="middle" width="300px"/>
        <figcaption align="middle">CB Gems</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

<h2 align="middle">Part 2: Bounding Volume Hierarchy</h2>
<p>
	As the number of primitive shapes—triangles and spheres, in our case—increases in more complex scenes, checking for intersection with each of these primitives for every generated ray becomes prohibitively expensive. The Bounding Volume Hierarchy (BVH) data structure allows us to check ray intersection in logarithmic time, rather than linear time, with respect to the total number of primitives in the scene. It leverages a binary tree of nodes defined by the 3D bounding box of the primitives they enclose. Within this framework, reaching a leaf node guarantees that only a small number of primitives remain, at which point checking all intersections becomes a constant-time operation.
</p>

<br>
<div align="middle">
  <table cellpadding="10">
    <tr>
      <td>
        <img src="images/part2-1.png" align="middle" width="300px"/>
        <figcaption align="middle">Root Node</figcaption>
      </td>
      <td>
        <img src="images/part2-2.png" align="middle" width="300px"/>
        <figcaption align="middle">Internal Node</figcaption>
      </td>
	  <td>
        <img src="images/part2-3.png" align="middle" width="300px"/>
        <figcaption align="middle">Leaf Node</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

<p>
	We opted to split internal nodes with the following heuristic. First, compute the average centroid of all primitives that the node encompasses, then partition these primitives into two sets based on their position relative to the average centroid along the most expansive axis of the bounding box. From here, we simply recurse on each of the two resulting subsets until encountering few enough primitives to create a leaf node.
</p>

<br>
<div align="middle">
  <table cellpadding="10">
    <tr>
      <td>
        <img src="images/part2-4.png" align="middle" width="300px"/>
        <figcaption align="middle">CB Lucy</figcaption>
      </td>
      <td>
        <img src="images/part2-5.png" align="middle" width="300px"/>
        <figcaption align="middle">Max Planck</figcaption>
      </td>
	  <td>
        <img src="images/part2-6.png" align="middle" width="300px"/>
        <figcaption align="middle">CB Dragon</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

<p>
	The efficiency of this data structure lies in the fact that we can quickly check whether a ray intersects the bounding box of a subtree, and then neglect to explore any further intersections therein if the ray misses this bounding box entirely. This splitting strategy produced significantly fewer intersection tests per ray than the naive approach we started with, by multiple orders of magnitude. Consequently, we are able to render images with more complex geometries like the ones above at a much faster rate. Below, we display the disparity in rendering times and intersection tests for some moderately complex scenes with and without a carefully constructed BVH backbone.
</p>

<br>
<div align="middle">
  <table cellpadding="10">
		<tr>
			<td>
				<img src="images/part2-7.png" align="middle" width="275px"/>
				<figcaption align="middle">
					<br>
					<b>Naive:</b> 6.795s render time
					<br>277.328 intersection tests per ray
					<br><br>
					<b>BVH:</b> 0.046s render time
					<br>1.819 intersection tests per ray
				</figcaption>
			</td>
			<td>
				<img src="images/part2-8.png" align="middle" width="275px"/>
				<figcaption align="middle">
					<br>
					<b>Naive:</b> 24.083s render time
					<br>912.353 intersection tests per ray
					<br><br>
					<b>BVH:</b> 0.033s render time
					<br>1.405 intersection tests per ray
				</figcaption>
			</td>
			<td>
				<img src="images/part2-9.png" align="middle" width="275px"/>
				<figcaption align="middle">
					<br>
					<b>Naive:</b> 21.360s render time
					<br>797.309 intersection tests per ray
					<br><br>
					<b>BVH:</b> 0.047s render time
					<br>2.379 intersection tests per ray
				</figcaption>
      </td>
    </tr>
	</table>
</div>
<br>

<h2 align="middle">Part 3: Direct Illumination</h2>
<p>
	With a faster system to compute ray intersections, we are equipped to start modeling more realistic shading in our scenes. Direct illumination is the combination of light emitted directly from light sources and the light that travels from a light source to an object in the scene, then bounces to the observer. Visually, direct illumination manifests as light falling directly onto the scene from all sources, as the name suggests.
</p>
<p>
	As we follow a ray backward through the scene and detect intersections with objects, we have to make a guess about which direction the ray came from and calculate the radiance coming from its origin in order to estimate how much light is reflected back to the camera. In this project, we approach this guessing game using two different sampling techniques. First, we create an imaginary unit hemisphere lying above the intersected point on our surface and create a direction vector, along which our ray will continue, by selecting a point on the hemisphere uniformly at random. We refer to this process as uniform hemisphere sampling. The other more clever method that we implemented is called importance sampling, which instead probabilistically weights the chosen direction vector to favor the locations of the light sources in the scene. Below, we compare the results of uniform hemisphere sampling and importance sampling using 16 samples per pixel and 8 samples per light source.
</p>

<br>
<div align="middle">
  <table cellpadding="10">
		<tr>
			<th align="middle"><b>Uniform Hemisphere Sampling</b></th>
			<th align="middle"><b>Importance Sampling</b></th>
		</tr>
    <tr>
      <td>
        <img src="images/part3-1.png" align="middle" width="300px"/>
      </td>
      <td>
        <img src="images/part3-2.png" align="middle" width="300px"/>
      </td>
    </tr>
		<tr>
			<th align="middle" colspan="2">CB Bunny</th>
		</tr>
		<tr>
			<td>
				<img src="images/part3-3.png" align="middle" width="300px"/>
			</td>
			<td>
				<img src="images/part3-4.png" align="middle" width="300px"/>
			</td>
		</tr>
		<tr>
			<th align="middle" colspan="2">CB Spheres</th>
		</tr>
  </table>
</div>
<br>

<p>
	We observe that uniform hemisphere sampling produces a very noisy output compared with importance sampling, given the relatively low number of samples with which we rendered the images. The reason for this disparity in quality is that uniform hemisphere sampling produces a lot of useless rays that bounce off arbitrarily into the void and do not contribute to the output. Importance sampling, on the other hand, ensures that most of the ray bounces we produce are directed toward a light source, which gives us meaningful output and allows us to converge to a reasonable image with far fewer samples. In the following images of the CB Bunny, we see how the noise levels in soft shadows improve quite rapidly as the number of samples per light source increases while using a lighting-sampled probability density function.
</p>

<br>
<div align="middle">
  <table cellpadding="10">
    <tr>
      <td>
        <img src="images/part3-5.png" align="middle" width="300px"/>
				<figcaption align="middle">1 Light Ray</figcaption>
      </td>
      <td>
        <img src="images/part3-6.png" align="middle" width="300px"/>
				<figcaption align="middle">2 Light Rays</figcaption>
      </td>
    </tr>
		<tr>
			<td>
				<img src="images/part3-7.png" align="middle" width="300px"/>
				<figcaption align="middle">16 Light Rays</figcaption>
			</td>
			<td>
				<img src="images/part3-8.png" align="middle" width="300px"/>
				<figcaption align="middle">64 Light Rays</figcaption>
			</td>
		</tr>
  </table>
</div>
<br>

<h2 align="middle">Part 4: Global Illumination</h2>
<p>
  Direct illumination on its own does not paint the full picture; we also need to account for light that bounces between objects and then onto the camera. Global illumination is an accumulation of direct and indirect illumination, representing a multitude of light rays bouncing across the scene, colliding with objects a variable number of times. Computationally, we view global illumination as tracing each casted ray between collisions until it either exits the scene indefinitely or terminates itself along the way, which therefore lends itself well to a recursive implementation.
</p>
<p>
  If the maximum ray depth is 1 or less, we simply simulate direct lighting according to our ideas in the previous section. Otherwise, indirect lighting has been activated and we need to determine whether the ray should continue to be traced throughout the scene. We employ a “Russian Roulette” scheme that randomly stops each ray from resuming its traversal with a 30% probability every time it hits a surface, in which case it to ascends the recursive stack and gathers the lighting it has accrued for the output pixel from which it started its journey.
</p>
<p>
	Remarkably, this Russian Roulette estimator for the infinite light bounces that would actually occur is unbiased. Intuitively, as the rays bounce more and more, their weighted radiance decreases from repeated reflection, so we enforce a maximum ray bounce depth to prevent visually negligible, yet expensive computation as well as the possibility of infinite recursion. The sample images below show a preview of the effect of global illumination.
</p>

<br>
<div align="middle">
  <table cellpadding="10">
    <tr>
      <td>
        <img src="images/part4-1.png" align="middle" width="300px"/>
				<figcaption align="middle">Bunny</figcaption>
      </td>
      <td>
        <img src="images/part4-2.png" align="middle" width="300px"/>
				<figcaption align="middle">Blob</figcaption>
      </td>
    </tr>
		<tr>
			<td>
				<img src="images/part4-3.png" align="middle" width="300px"/>
				<figcaption align="middle">Bench</figcaption>
			</td>
			<td>
				<img src="images/part4-4-19.png" align="middle" width="300px"/>
				<figcaption align="middle">Dragon</figcaption>
			</td>
		</tr>
  </table>
</div>
<br>

<p>
	Because the global illumination process is purely additive in terms of radiance, we can separate the direct and indirect illumination of a rendered image. Direct illumination highlights the light source and the components of scene objects directly facing the light source with harsh lighting and shadows. Indirect illumination, however, casts light about the scene more evenly, as rays are given more free rein to bounce around and land on a wider variety of locations, including those that direct lighting rays cannot reach. This produces more convincing diffuse lighting and causes colors to be reflected between objects, most noticeably between the colored walls and the white spheres in the example below.
</p>

<br>
<div align="middle">
  <table cellpadding="10">
    <tr>
      <td>
        <img src="images/part4-5.png" align="middle" width="300px"/>
				<figcaption align="middle">Direct Lighting</figcaption>
      </td>
      <td>
        <img src="images/part4-6.png" align="middle" width="300px"/>
				<figcaption align="middle">Indirect Lighting</figcaption>
      </td>
			<td>
        <img src="images/part4-7.png" align="middle" width="300px"/>
				<figcaption align="middle">Global Illumination</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

<p>
	In the next progression of images, we observe how increasing the maximum ray depth affects global illumination. With zero-bounce radiance, we have only the light source visible. When the maximum ray depth is one, we obtain direct lighting as expected. After 2 bounces, we consider indirect lighting in addition to direct lighting for the first time, causing the ceiling of the box to be lit and the shadows to become much softer. With a maximum ray depth of 3, the resulting image is quite similar to the previous one, but with slightly lighter shadows that are most obvious in the top corners and edges of the room. Using a maximum depth of 100, the shadows in the top corners and edges recede a little bit more, and the colors reflected by the walls become more pronounced, especially in the shadows near them. The lack of a more substantial difference between maximum depths of 3 and 100 is likely caused by Russian Roulette killing off most rays much earlier than 100 bounces.
</p>

<br>
<div align="middle">
  <table cellpadding="10">
    <tr>
      <td>
        <img src="images/part4-8.png" align="middle" width="300px"/>
				<figcaption align="middle">Max Depth = 0</figcaption>
      </td>
      <td>
        <img src="images/part4-9.png" align="middle" width="300px"/>
				<figcaption align="middle">Max Depth = 1</figcaption>
      </td>
			<td>
        <img src="images/part4-10.png" align="middle" width="300px"/>
				<figcaption align="middle">Max Depth = 2</figcaption>
      </td>
    </tr>
  </table>
</div>
<div align="middle">
  <table cellpadding="10">
    <tr>
      <td>
        <img src="images/part4-11.png" align="middle" width="300px"/>
				<figcaption align="middle">Max Depth = 3</figcaption>
      </td>
      <td>
        <img src="images/part4-12.png" align="middle" width="300px"/>
				<figcaption align="middle">Max Depth = 100</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

<p>
	Now, we render the dragon image using varying samples per pixel with a fixed 4 samples per light. We see below that the images become smoother, more evenly lit, and overall less noisy as we increase the number of samples per pixel.
</p>

<br>
<div align="middle">
  <table cellpadding="10">
    <tr>
      <td>
        <img src="images/part4-13.png" align="middle" width="225px"/>
				<figcaption align="middle">1 Sample Per Pixel</figcaption>
      </td>
      <td>
        <img src="images/part4-14.png" align="middle" width="225px"/>
				<figcaption align="middle">2 Samples Per Pixel</figcaption>
      </td>
			<td>
        <img src="images/part4-15.png" align="middle" width="225px"/>
				<figcaption align="middle">4 Samples Per Pixel</figcaption>
      </td>
			<td>
        <img src="images/part4-16.png" align="middle" width="225px"/>
				<figcaption align="middle">8 Samples Per Pixel</figcaption>
      </td>
    </tr>
  </table>
</div>
<div align="middle">
  <table cellpadding="10">
    <tr>
      <td>
        <img src="images/part4-17.png" align="middle" width="225px"/>
				<figcaption align="middle">16 Samples Per Pixel</figcaption>
      </td>
      <td>
        <img src="images/part4-18.png" align="middle" width="225px"/>
				<figcaption align="middle">64 Samples Per Pixel</figcaption>
      </td>
			<td>
        <img src="images/part4-4-19.png" align="middle" width="225px"/>
				<figcaption align="middle">1024 Samples Per Pixel</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

<h3 align="middle">Part 5: Adaptive Sampling</h3>
<p>
  Until this point, we have been using a constant number of ray samples for every pixel while performing our path-tracing algorithm, perhaps wastefully so. Adaptive sampling address this dilemma by sampling randomly within each pixel until it converges to a particular value, up to some allowable tolerance, at which point we can safely terminate our sampling for the pixel in question. If a pixel's value takes too long to converge, the maximum number of samples specified is taken. This scheme helps improve rendering times by decreasing the total number of ray samples required.
</p>
<p>
  To implement adaptive sampling, we maintain a running mean and variance of all of the sampled values so far for each pixel. After completing a batch of samples (of configurable size), we calculate the pixel's convergence value and compare it to the product of the running mean and the maximum tolerance. If the convergence is less than this acceptable bound, we stop sampling for that pixel. Otherwise, we continue until we reach either convergence or the maximum number of samples. Effectively, this procedure checks if new pixel samples vary significantly from the running mean and proceeds accordingly.
</p>
<p>
	Below, we visualize sampling rate heatmaps for a couple of scenes, where red represents a high rate, green corresponds to a medium rate, and blue depicts a low rate. We note that direct lighting zones require few samples relative to indirect lighting zones.
</p>

<br>
<div align="middle">
  <table cellpadding="10">
		<tr>
			<th align="middle"><b>Rendered Image</b></th>
			<th align="middle"><b>Sampling Rate</b></th>
		</tr>
    <tr>
      <td>
        <img src="images/part5-1.png" align="middle" width="300px"/>
      </td>
      <td>
        <img src="images/part5-2.png" align="middle" width="300px"/>
      </td>
    </tr>
		<tr>
			<th align="middle" colspan="2">CB Bunny</th>
		</tr>
		<tr>
			<td>
				<img src="images/part5-3.png" align="middle" width="300px"/>
			</td>
			<td>
				<img src="images/part5-4.png" align="middle" width="300px"/>
			</td>
		</tr>
		<tr>
			<th align="middle" colspan="2">CB Spheres</th>
		</tr>
  </table>
</div>
<br>

<h3>Webpage Link:</h3>
<a href=https://martinhodde.github.io/cs184-proj-webpage/proj3-1>martinhodde.github.io/cs184-proj-webpage/proj3-1</a>
</body>
</html>
